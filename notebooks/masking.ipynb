{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_attention_mask(\n",
    "    q_tokens: torch.Tensor,\n",
    "    k_tokens: torch.Tensor,\n",
    "    q_pad_idx: int | None,\n",
    "    k_pad_idx: int | None,\n",
    "    mask_future: bool,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Returns an addative attention mask.\"\"\"\n",
    "    B, T = q_tokens.shape\n",
    "    mask = torch.zeros((B, T, T), dtype=torch.float32)\n",
    "\n",
    "    # Pad mask.\n",
    "    if None not in (q_pad_idx, k_pad_idx):\n",
    "        q_mask = (q_tokens == q_pad_idx).unsqueeze(-1)\n",
    "        k_mask = (k_tokens == k_pad_idx).unsqueeze(-2)\n",
    "        print(q_mask.shape, k_mask.shape)\n",
    "        mask[q_mask | k_mask] = torch.finfo(torch.float32).min\n",
    "\n",
    "    # Future mask.\n",
    "    if mask_future:\n",
    "        future_mask = torch.triu(\n",
    "            torch.full((T, T), fill_value=True, dtype=torch.bool), diagonal=1\n",
    "        )\n",
    "        mask.masked_fill_(future_mask, float(\"-inf\"))\n",
    "\n",
    "    print(mask.shape)\n",
    "\n",
    "    return mask.to(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "tensor([[[ 0.0000e+00,        -inf,        -inf,        -inf,        -inf],\n",
      "         [ 0.0000e+00,  0.0000e+00,        -inf,        -inf,        -inf],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,        -inf,        -inf],\n",
      "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,        -inf],\n",
      "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38]]])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6311, 0.3689, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3140, 0.4101, 0.2759, 0.0000, 0.0000],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "          [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5800, 0.4200, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3043, 0.3177, 0.3780, 0.0000, 0.0000],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "          [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4259, 0.5741, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3561, 0.4069, 0.2370, 0.0000, 0.0000],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "          [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5366, 0.4634, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3602, 0.2880, 0.3518, 0.0000, 0.0000],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "          [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "q_tokens = torch.tensor([[1, 2, 3, 5, 5]])\n",
    "k_tokens = torch.tensor([[1, 2, 3, 4, 7]])\n",
    "\n",
    "attention_mask = get_attention_mask(\n",
    "    q_tokens, k_tokens, q_pad_idx=5, k_pad_idx=7, mask_future=True\n",
    ")\n",
    "print(attention_mask)\n",
    "\n",
    "wei = torch.rand((2, 2, 5, 5))\n",
    "\n",
    "wei = wei + attention_mask\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
